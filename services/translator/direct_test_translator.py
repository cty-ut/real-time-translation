
import logging
from transformers import MarianMTModel, MarianTokenizer
import torch

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_direct_test():
    """
    ç›´æ¥åœ¨å®¹å™¨å†…æµ‹è¯•ç¿»è¯‘æ¨¡å‹çš„åŠ è½½å’Œç¿»è¯‘åŠŸèƒ½ã€‚
    """
    model_name = "Helsinki-NLP/opus-mt-en-zh"
    device = "cpu"  # å®¹å™¨å†…é€šå¸¸æ²¡æœ‰ GPUï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU
    
    # --- 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    try:
        logger.info(f"æ­£åœ¨åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_name} on {device}")
        # ä½¿ç”¨ä¸ä¸»åº”ç”¨ç›¸åŒçš„æ¨¡å‹ç¼“å­˜è·¯å¾„
        tokenizer = MarianTokenizer.from_pretrained(model_name, cache_dir="/app/models")
        model = MarianMTModel.from_pretrained(model_name, cache_dir="/app/models")
        model.to(device)
        logger.info("âœ… ç¿»è¯‘æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
        return

    # --- 2. æ‰§è¡Œç¿»è¯‘ ---
    try:
        text_to_translate = "Hello, world!"
        logger.info(f"æ­£åœ¨ç¿»è¯‘æ–‡æœ¬: '{text_to_translate}'")
        
        # å‡†å¤‡è¾“å…¥
        inputs = tokenizer(text_to_translate, return_tensors="pt", padding=True).to(device)
        
        # ç”Ÿæˆç¿»è¯‘ç»“æœ
        translated_tokens = model.generate(**inputs)
        
        # è§£ç ç»“æœ
        translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
        
        logger.info("âœ… ç¿»è¯‘ä»»åŠ¡å®Œæˆã€‚")
        logger.info("="*20 + " ç¿»è¯‘ç»“æœ " + "="*20)
        logger.info(f"åŸæ–‡ (en): {text_to_translate}")
        logger.info(f"è¯‘æ–‡ (zh): {translated_text}")
        logger.info("="*50)
        logger.info("ğŸ‰ æ¨¡å‹ç›´æ¥æµ‹è¯•æˆåŠŸï¼")

    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

if __name__ == "__main__":
    run_direct_test()
