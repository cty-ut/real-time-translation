import React, { useState, useRef, useEffect } from 'react';
import styled from 'styled-components';

const Container = styled.div`
  padding: 20px;
  text-align: center;
`;

const VisualizerContainer = styled.div`
  display: flex;
  justify-content: center;
  align-items: center;
  height: 80px;
  margin: 20px 0;
  background: rgba(0, 0, 0, 0.05);
  border-radius: 10px;
  padding: 20px;
`;

const AudioBar = styled.div`
  width: 4px;
  background: ${props => props.isActive ? '#4CAF50' : '#ccc'};
  border-radius: 2px;
  margin: 0 1px;
  transition: height 0.1s ease;
  height: ${props => props.height}px;
`;

const StatusText = styled.p`
  margin: 10px 0;
  color: ${props => props.isListening ? '#4CAF50' : '#666'};
  font-weight: 500;
`;

const ProgressBar = styled.div`
  width: 100%;
  height: 4px;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  overflow: hidden;
  margin: 10px 0;
`;

const Progress = styled.div`
  height: 100%;
  background: #4CAF50;
  border-radius: 2px;
  transition: width 0.3s ease;
  width: ${props => props.progress}%;
`;

const AudioCapture = ({ 
  isActive, 
  language, 
  onError,
  socket,
  targetLanguage = 'zh',
  lastTranscription // æ–°å¢ props
}) => {
  const [mediaRecorder, setMediaRecorder] = useState(null);
  const [audioLevel, setAudioLevel] = useState(0);
  const [isRecording, setIsRecording] = useState(false);
  const [processingProgress, setProcessingProgress] = useState(0);
  const [sessionId, setSessionId] = useState(null);

  const audioChunks = useRef([]);
  const audioContext = useRef(null);
  const analyser = useRef(null);
  const microphone = useRef(null);
  const dataArray = useRef(null);
  const animationFrame = useRef(null);
  const silenceTimer = useRef(null);
  const recordingTimer = useRef(null);
  const mediaStream = useRef(null);

  // é…ç½® - ä¼˜åŒ–å®æ—¶æ€§
  const SILENCE_THRESHOLD = 0.005;
  const SILENCE_DURATION = 700;    // é™éŸ³æ£€æµ‹æ—¶é•¿ (ms)
  const MAX_RECORDING_DURATION = 5000;  // æœ€å¤§å½•éŸ³æ—¶é•¿ (ms)
  const MIN_RECORDING_DURATION = 500;   // æœ€å°å½•éŸ³æ—¶é•¿ (ms)
  const MIN_AUDIO_SIZE = 1000;  // æœ€å°éŸ³é¢‘æ–‡ä»¶å¤§å° (bytes)

  // åˆå§‹åŒ–éŸ³é¢‘è®¾å¤‡
  useEffect(() => {
    if (isActive) {
      initializeAudio();
    } else {
      cleanup();
    }

    return cleanup;
  }, [isActive]);

  const initializeAudio = async () => {
    try {
      // æ¸…ç†ä¹‹å‰çš„èµ„æº
      cleanup();
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 16000
        }
      });

      if (stream.getAudioTracks().length === 0) {
        throw new Error('æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘è½¨é“');
      }

      // ä¿å­˜åª’ä½“æµå¼•ç”¨
      mediaStream.current = stream;

      // è®¾ç½®éŸ³é¢‘åˆ†æ
      audioContext.current = new (window.AudioContext || window.webkitAudioContext)();
      
      // ç¡®ä¿AudioContextå¤„äºè¿è¡ŒçŠ¶æ€
      if (audioContext.current.state === 'suspended') {
        await audioContext.current.resume();
      }
      
      analyser.current = audioContext.current.createAnalyser();
      microphone.current = audioContext.current.createMediaStreamSource(stream);
      
      // ä¼˜åŒ–éŸ³é¢‘åˆ†æå™¨è®¾ç½®ä»¥æé«˜æ•æ„Ÿåº¦
      analyser.current.fftSize = 512;
      analyser.current.smoothingTimeConstant = 0.3;
      analyser.current.minDecibels = -90;
      analyser.current.maxDecibels = -10;
      
      microphone.current.connect(analyser.current);
      
      const bufferLength = analyser.current.frequencyBinCount;
      dataArray.current = new Uint8Array(bufferLength);

      // è®¾ç½®å½•éŸ³å™¨ - ä¼˜åŒ–éŸ³é¢‘æ ¼å¼å…¼å®¹æ€§
      let options = {};
      
      // æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„éŸ³é¢‘æ ¼å¼
      const mimeTypes = [
        'audio/webm;codecs=opus',
        'audio/webm;codecs=vp8,opus',
        'audio/webm',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/wav'
      ];
      
      for (const mimeType of mimeTypes) {
        if (MediaRecorder.isTypeSupported(mimeType)) {
          options = { 
            mimeType,
            audioBitsPerSecond: 16000  // è®¾ç½®éŸ³é¢‘æ¯”ç‰¹ç‡
          };
          console.log(`ä½¿ç”¨éŸ³é¢‘æ ¼å¼: ${mimeType}`);
          break;
        }
      }
      
      if (!options.mimeType) {
        console.log('ä½¿ç”¨é»˜è®¤éŸ³é¢‘æ ¼å¼');
        options = {
          audioBitsPerSecond: 16000
        };
      }
      
      const recorder = new MediaRecorder(stream, options);

      recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.current.push(event.data);
        }
      };

      recorder.onstop = () => {
        processAudioChunks();
      };

      recorder.onerror = (event) => {
        onError('å½•éŸ³å™¨é”™è¯¯: ' + event.error.message);
      };

      // ä½¿ç”¨å›è°ƒç¡®ä¿çŠ¶æ€æ›´æ–°åå†å¼€å§‹å½•éŸ³
      setMediaRecorder(() => {
        // å»¶è¿Ÿå¯åŠ¨ä»¥ç¡®ä¿çŠ¶æ€å·²æ›´æ–°
        setTimeout(() => {
          if (isActive) {
            startListeningWithRecorder(recorder);
          }
        }, 100);
        return recorder;
      });

    } catch (error) {
      onError('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®: ' + error.message);
    }
  };

  const startListeningWithRecorder = (recorder) => {
    if (!recorder || recorder.state === 'recording') {
      return;
    }

    audioChunks.current = [];
    setIsRecording(true);
    
    // è®°å½•å½•åˆ¶å¼€å§‹æ—¶é—´
    window.recordingStartTime = Date.now();
    
    try {
      // è®¾ç½®å½•éŸ³å‚æ•° - ä½¿ç”¨æ›´çŸ­çš„æ—¶é—´ç‰‡
      recorder.start(100); // å‡å°‘åˆ°100msæé«˜å“åº”æ€§
    } catch (error) {
      onError('å¯åŠ¨å½•éŸ³å¤±è´¥: ' + error.message);
      return;
    }
    
    // å¼€å§‹éŸ³é¢‘å¯è§†åŒ–
    visualizeAudio();
    
    // è®¾ç½®æœ€å¤§å½•éŸ³æ—¶é•¿
    recordingTimer.current = setTimeout(() => {
      if (recorder.state === 'recording') {
        stopRecording();
      }
    }, MAX_RECORDING_DURATION);
  };

  const startListening = () => {
    if (!mediaRecorder) {
      return;
    }

    startListeningWithRecorder(mediaRecorder);
  };

  const stopRecording = () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop();
      setIsRecording(false);
      
      if (recordingTimer.current) {
        clearTimeout(recordingTimer.current);
        recordingTimer.current = null;
      }
      
      if (silenceTimer.current) {
        clearTimeout(silenceTimer.current);
        silenceTimer.current = null;
      }
    }
  };

  const visualizeAudio = () => {
    if (!analyser.current || !dataArray.current || !isActive) {
      return;
    }

    // ä½¿ç”¨ä¸¤ç§æ–¹æ³•æ£€æµ‹éŸ³é¢‘ï¼šé¢‘åŸŸå’Œæ—¶åŸŸ
    
    // æ–¹æ³•1ï¼šé¢‘åŸŸåˆ†æ (åŸæœ‰æ–¹æ³•)
    analyser.current.getByteFrequencyData(dataArray.current);
    const freqAverage = dataArray.current.reduce((a, b) => a + b) / dataArray.current.length;
    const freqLevel = freqAverage / 255;
    
    // æ–¹æ³•2ï¼šæ—¶åŸŸåˆ†æ (æ›´æ•æ„Ÿ)
    const timeDataArray = new Uint8Array(analyser.current.fftSize);
    analyser.current.getByteTimeDomainData(timeDataArray);
    
    // è®¡ç®—RMS (å‡æ–¹æ ¹) æ¥æ£€æµ‹éŸ³é¢‘å¼ºåº¦
    let sumSquares = 0;
    for (let i = 0; i < timeDataArray.length; i++) {
      const normalized = (timeDataArray[i] - 128) / 128;
      sumSquares += normalized * normalized;
    }
    const rms = Math.sqrt(sumSquares / timeDataArray.length);
    
    // ä½¿ç”¨ä¸¤ç§æ–¹æ³•ä¸­è¾ƒé«˜çš„ä¸€ä¸ª
    const normalizedLevel = Math.max(freqLevel, rms);
    
    setAudioLevel(normalizedLevel);

    // æ£€æµ‹é™éŸ³
    if (normalizedLevel < SILENCE_THRESHOLD) {
      if (!silenceTimer.current && isRecording) {
        silenceTimer.current = setTimeout(() => {
          // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°å½•åˆ¶æ—¶é•¿
          const recordingDuration = Date.now() - (window.recordingStartTime || 0);
          if (recordingDuration >= MIN_RECORDING_DURATION) {
            stopRecording();
          } else {
            // æœªè¾¾åˆ°æœ€å°æ—¶é•¿ï¼Œç»§ç»­å½•åˆ¶
            silenceTimer.current = null;
          }
        }, SILENCE_DURATION);
      }
    } else {
      if (silenceTimer.current) {
        clearTimeout(silenceTimer.current);
        silenceTimer.current = null;
      }
    }

    if (isActive) {
      animationFrame.current = requestAnimationFrame(visualizeAudio);
    }
  };

  const processAudioChunks = async () => {
    if (audioChunks.current.length === 0) {
      setTimeout(startListening, 500);
      return;
    }

    try {
      setProcessingProgress(10);
      
      const audioBlob = new Blob(audioChunks.current, { type: audioChunks.current[0]?.type || 'audio/webm' });
      
      // æ£€æŸ¥éŸ³é¢‘æ—¶é•¿å’Œå¤§å° - ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ£€æŸ¥
      if (audioBlob.size < MIN_AUDIO_SIZE) {
        console.log(`éŸ³é¢‘æ–‡ä»¶å¤ªå° (${audioBlob.size} bytes)ï¼Œè·³è¿‡å¤„ç†`);
        setTimeout(startListening, 500);
        return;
      }

      const arrayBuffer = await audioBlob.arrayBuffer();
      if (arrayBuffer.byteLength === 0) {
        throw new Error('éŸ³é¢‘æ•°æ®ä¸ºç©º');
      }
      
      setProcessingProgress(30);

      if (socket && socket.connected) {
        const currentSessionId = Date.now().toString();
        setSessionId(currentSessionId);
        sendAudioForTranscription(arrayBuffer, audioBlob.type, currentSessionId);
        setProcessingProgress(100); // Assume sent
        setTimeout(() => {
          setProcessingProgress(0);
          if (isActive) {
            startListening();
          }
        }, 200);
      } else {
        onError('WebSocket æœªè¿æ¥');
        setProcessingProgress(0);
        if (isActive) {
          setTimeout(startListening, 1000);
        }
      }

    } catch (error) {
      onError('éŸ³é¢‘å¤„ç†å¤±è´¥: ' + error.message);
      setProcessingProgress(0);
      if (isActive) {
        setTimeout(startListening, 1000);
      }
    }
  };

  const sendAudioForTranscription = (audioData, mimeType, sessionId) => {
    if (!socket || !socket.connected) {
      onError('WebSocket æœªè¿æ¥ï¼Œæ— æ³•å‘é€éŸ³é¢‘');
      return;
    }

    socket.emit('audio_chunk', {
      audio: audioData,
      language: language,
      target_lang: targetLanguage,
      sessionId: sessionId,
      mimeType: mimeType || mediaRecorder?.mimeType || 'audio/webm'
    });

    setProcessingProgress(50); // æ›´æ–°è¿›åº¦æ¡
  };

  const cleanup = () => {
    // æ¸…ç†åŠ¨ç”»å¸§
    if (animationFrame.current) {
      cancelAnimationFrame(animationFrame.current);
      animationFrame.current = null;
    }
    
    // æ¸…ç†å®šæ—¶å™¨
    if (silenceTimer.current) {
      clearTimeout(silenceTimer.current);
      silenceTimer.current = null;
    }
    
    if (recordingTimer.current) {
      clearTimeout(recordingTimer.current);
      recordingTimer.current = null;
    }

    // åœæ­¢å½•éŸ³
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      try {
        mediaRecorder.stop();
      } catch (error) {
        // Ignore
      }
    }

    // æ–­å¼€éŸ³é¢‘èŠ‚ç‚¹è¿æ¥
    if (microphone.current) {
      try {
        microphone.current.disconnect();
        microphone.current = null;
      } catch (error) {
        // Ignore
      }
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContext.current && audioContext.current.state !== 'closed') {
      try {
        audioContext.current.close();
        audioContext.current = null;
      } catch (error) {
        // Ignore
      }
    }

    // åœæ­¢åª’ä½“æµ
    if (mediaStream.current) {
      try {
        mediaStream.current.getTracks().forEach(track => {
          track.stop();
        });
        mediaStream.current = null;
      } catch (error) {
        // Ignore
      }
    }

    // é‡ç½®çŠ¶æ€
    setIsRecording(false);
    setAudioLevel(0);
    setProcessingProgress(0);
    setMediaRecorder(null);
    
    // æ¸…ç†éŸ³é¢‘æ•°æ®
    audioChunks.current = [];
    dataArray.current = null;
    analyser.current = null;
  };

  // ç”Ÿæˆå¯è§†åŒ–æŸ±çŠ¶å›¾
  const generateVisualizerBars = () => {
    const bars = [];
    const barCount = 20;
    
    for (let i = 0; i < barCount; i++) {
      const height = isRecording 
        ? 10 + Math.random() * audioLevel * 50 
        : 10;
      
      bars.push(
        <AudioBar
          key={i}
          height={height}
          isActive={isRecording && audioLevel > 0.01}
        />
      );
    }
    
    return bars;
  };

  return (
    <Container>
      <StatusText isListening={isRecording}>
        {isRecording ? 'ğŸ¤ æ­£åœ¨ç›‘å¬...' : 
         processingProgress > 0 ? 'ğŸ”„ å¤„ç†ä¸­...' : 
         'â¸ï¸ ç­‰å¾…ä¸­...'}
      </StatusText>

      <VisualizerContainer>
        {generateVisualizerBars()}
      </VisualizerContainer>

      {processingProgress > 0 && (
        <ProgressBar>
          <Progress progress={processingProgress} />
        </ProgressBar>
      )}

      {lastTranscription && (
        <StatusText style={{ color: '#666', fontSize: '14px', fontStyle: 'italic' }}>
          æœ€åè¯†åˆ«: {lastTranscription}
        </StatusText>
      )}
    </Container>
  );
};

export default AudioCapture;
